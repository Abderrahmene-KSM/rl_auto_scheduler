tiramisu:
    tiramisu_path: "/data/ak11326/Tools/tiramisu"
    env_type: "execution" # model | execution
    tags_model_weights: "/data/ak11326/Dev/rl_auto_scheduler_2/env_api/scheduler/models/cost_model_weights.pt"
    is_new_tiramisu: True
    workspace: "/data/ak11326/Dev/tiramisu_workspace/"
    old_tiramisu_path: "/data/ak11326/Tools/tiramisu"

dataset:
    dataset_format: PICKLE
    cpps_path : "/data/ak11326/Dev/datasets/cpps_mr_nr_ts.pkl"
    dataset_path: "/data/ak11326/Dev/datasets/data_mr_nr_ts.pkl"
    save_path: "/data/ak11326/Dev/datasets"
    shuffle: True
    seed: 7
    saving_frequency: 500
    # When doing evaluation on the benchmark set the value to True
    is_benchmark: False

ray:
    results: "/data/ak11326/Dev/tiramisu_workspace/results"
    restore_checkpoint: ""

experiment:
    name: "fixed-bugs"
    # name: "220k-1.5vf_coeff"
    checkpoint_frequency: 20
    checkpoint_num_to_keep: 30
    # The following 3 values are the values to stop the experiment if any of them is reached 
    training_iteration: 80000
    timesteps_total: 10000000
    episode_reward_mean: 10
    # Use this value to punish or tolerate illegal actions from being taken
    legality_speedup: 1.0
    # Use the order of beam search 
    beam_search_order : True
    entropy_coeff: 0.0001
    # Training parameters
    train_batch_size: 4096
    minibatch_size: 128
    lr: 0.000004
    vf_loss_coeff: 1
    # Policy model type
    policy_model: "lstm" #| "ff" #feed-forward
    # In mode "ff" set vf_share_layers to True if you want to use shared weights between policy and value function 
    # in mode "lstm" , you must make vf_share_layers = True
    vf_share_layers: True

policy_network:
    policy_hidden_layers: 
        - 2048
        - 512 
        - 64
    # If vf_share_layers is true then, these values won't be taken for the value network
    vf_hidden_layers: 
        - 512 
        - 64
    dropout_rate: 0.2

lstm_policy:
    fc_size: 1024
    lstm_state_size: 512
    num_layers: 1
    
